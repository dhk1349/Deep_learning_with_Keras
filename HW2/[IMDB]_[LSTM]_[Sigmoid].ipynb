{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>IMDB dateset</h2>\n",
    "\n",
    "IMDB는 (영화 리뷰, 리뷰의 긍정부정 값)을 한 쌍의 데이터 값으로, 50,000개의 샘플을 모아 놓은 데이터 셋이다. 이 중 25,000개의 댓글 셈플은 트레이닝 샘플이며, 나머지 25,000개의 댓글 샘플은 테스팅 샘플이다.<br>\n",
    "다시말해, 해당 데이터 셋에는 50000개의 영화 리뷰를 가지고 있으며, 모든 리뷰들은 결과 값(부정 혹은 긍적) 값을 결과 값으로 가지고 있다. \n",
    "해당 데이터 셋의 결과 값이 이진 데이터라는 특성 때문에 해당 데이터 셋을 사용하는 신경망 모델의 목적은 주로 binary classification을 목적으로 한다.\n",
    "\n",
    "해당 파일에서는 편의를 위해 데이터를 로드할 때, 가장 빈번하게 사용되는 20,000개의 단어들만 포함하여 로드하였다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 내부에서 training을 위한 데이터와 이를 각 epoch에서 검증하기 위한 데이터를 분류\n",
    "하기 위해 25,000개의 데이터를 분할하였다.<br>\n",
    "25,000개이 샘플 중 20,000개는 트레이닝을 위한 샘플, 나머지 5,000개는 트레이닝 후 정확도 평가를 위한 validation 샘플로 두었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[20000:]\n",
    "y_val = y_train[20000:]\n",
    "x_train = x_train[:20000]\n",
    "y_train = y_train[:20000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>pad_sequence() function</h4>\n",
    "pad_sequence()라는 함수는 케라스에 내장되어 있는 함수로, 데이터 전처리를 하는 것이 그 용도이다. </br>\n",
    "해당 함수를 사용하면, 샘플 간의 길이를 균등하게 맞출 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=200)\n",
    "x_val = sequence.pad_sequences(x_val, maxlen=200)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>모델 구성</h2>\n",
    "Sequential() 함수로 모델을 초기화 한다.\n",
    "\n",
    "<h4>Embedding()층을 추가한다</h4>\n",
    "Embedding()은 one-hot-encoding보다 훨씬 적은 공간을 사용한다. 또한 해당 층을 추가하면 단어 간의 연관성을 고려한 벡터가 만들어지기 때문에 훨씬 유의미한 훈련이 가능하다.\n",
    "\n",
    "<h4>LSTM</h4>\n",
    "LSTM은 Long Short-term Memory의 약자이다. 이는 기본적인 RNN 신경망 구조에서 변형된 형태이다. 기존의 RNN은 입력값의 시퀀스 길이가 길어질 때, 훈련이 반영이 잘 되지 않는 \"Vanishing gradient\"문제 떄문에 해당 모델이 구상되었다.<br>\n",
    "해당 모델은 훈련이 거듭될 때마다 결과 값을 기억하고 있는다. 다만, 새로운 입력값과 기억하고 있는 데이터의 연관 정도를 고려하여 기억 데이터의 일부분을 삭제하고 새로운 입력 값을 기억한다는 적이 해당 기법의 특이한 점이다. \n",
    "\n",
    "<h4>출력층: Sigmoid 함수</h4>\n",
    "해당 모델의 목적은 입력되는 리뷰의 긍정 부정을 판별하는 것이다. Binary classification에 적합한 Sigmoid 함수를 사용하였다.\n",
    "\n",
    "\n",
    "<h4>Adam optimizer</h4>\n",
    "<p>\n",
    "    ADAM stands for Adaptive Moment Estimation\n",
    "ADAM is gradient descent using both Momentum gradient descent and RMprop gradient descent.<br></p>\n",
    "    <h4>Argument</h4>\n",
    "<p>\n",
    "lr: float >= 0. Learning rate.<br>\n",
    "beta_1: float, 0 < beta < 1. Generally close to 1.<br>\n",
    "beta_2: float, 0 < beta < 1. Generally close to 1.<br>\n",
    "epsilon: float >= 0. Fuzz factor. If None, defaults to K.epsilon().<br>\n",
    "decay: float >= 0. Learning rate decay over each update.<br>\n",
    "amsgrad: boolean. Whether to apply the AMSGrad variant of this algorithm from the paper \"On the Convergence of Adam and Beyond\".<br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dhk13\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,691,713\n",
      "Trainable params: 2,691,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\dhk13\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/25\n",
      "20000/20000 [==============================] - 109s 5ms/step - loss: 0.4625 - acc: 0.7827 - val_loss: 0.4485 - val_acc: 0.7982\n",
      "Epoch 2/25\n",
      "20000/20000 [==============================] - 109s 5ms/step - loss: 0.2447 - acc: 0.9058 - val_loss: 0.3220 - val_acc: 0.8708\n",
      "Epoch 3/25\n",
      "20000/20000 [==============================] - 112s 6ms/step - loss: 0.1500 - acc: 0.9468 - val_loss: 0.4150 - val_acc: 0.8532\n",
      "Epoch 4/25\n",
      "20000/20000 [==============================] - 106s 5ms/step - loss: 0.0851 - acc: 0.9714 - val_loss: 0.4705 - val_acc: 0.8678\n",
      "Epoch 5/25\n",
      "20000/20000 [==============================] - 104s 5ms/step - loss: 0.0578 - acc: 0.9815 - val_loss: 0.5326 - val_acc: 0.8630\n",
      "Epoch 6/25\n",
      "20000/20000 [==============================] - 104s 5ms/step - loss: 0.0615 - acc: 0.9796 - val_loss: 0.5466 - val_acc: 0.8372\n",
      "Epoch 7/25\n",
      "20000/20000 [==============================] - 103s 5ms/step - loss: 0.0320 - acc: 0.9904 - val_loss: 0.5614 - val_acc: 0.8318\n",
      "Epoch 8/25\n",
      "20000/20000 [==============================] - 100s 5ms/step - loss: 0.0324 - acc: 0.9901 - val_loss: 0.5965 - val_acc: 0.8444\n",
      "Epoch 9/25\n",
      "20000/20000 [==============================] - 97s 5ms/step - loss: 0.0475 - acc: 0.9832 - val_loss: 0.6446 - val_acc: 0.8466\n",
      "Epoch 10/25\n",
      "20000/20000 [==============================] - 102s 5ms/step - loss: 0.0324 - acc: 0.9886 - val_loss: 0.6257 - val_acc: 0.8336\n",
      "Epoch 11/25\n",
      "20000/20000 [==============================] - 100s 5ms/step - loss: 0.0270 - acc: 0.9924 - val_loss: 0.7276 - val_acc: 0.8526\n",
      "Epoch 12/25\n",
      "20000/20000 [==============================] - 102s 5ms/step - loss: 0.0090 - acc: 0.9979 - val_loss: 0.8802 - val_acc: 0.8506\n",
      "Epoch 13/25\n",
      "20000/20000 [==============================] - 98s 5ms/step - loss: 0.0277 - acc: 0.9902 - val_loss: 0.9150 - val_acc: 0.8010\n",
      "Epoch 14/25\n",
      "20000/20000 [==============================] - 101s 5ms/step - loss: 0.0199 - acc: 0.9935 - val_loss: 0.7912 - val_acc: 0.8332\n",
      "Epoch 15/25\n",
      "20000/20000 [==============================] - 101s 5ms/step - loss: 0.0083 - acc: 0.9974 - val_loss: 0.8311 - val_acc: 0.8454\n",
      "Epoch 16/25\n",
      "20000/20000 [==============================] - 101s 5ms/step - loss: 0.0023 - acc: 0.9996 - val_loss: 0.9164 - val_acc: 0.8556\n",
      "Epoch 17/25\n",
      "20000/20000 [==============================] - 100s 5ms/step - loss: 5.5037e-04 - acc: 1.0000 - val_loss: 1.0446 - val_acc: 0.8574\n",
      "Epoch 18/25\n",
      "20000/20000 [==============================] - 99s 5ms/step - loss: 1.2941e-04 - acc: 1.0000 - val_loss: 1.0906 - val_acc: 0.8566\n",
      "Epoch 19/25\n",
      "20000/20000 [==============================] - 100s 5ms/step - loss: 7.3302e-05 - acc: 1.0000 - val_loss: 1.1404 - val_acc: 0.8578\n",
      "Epoch 20/25\n",
      "20000/20000 [==============================] - 102s 5ms/step - loss: 4.9377e-05 - acc: 1.0000 - val_loss: 1.1733 - val_acc: 0.8568\n",
      "Epoch 21/25\n",
      "20000/20000 [==============================] - 103s 5ms/step - loss: 3.5746e-05 - acc: 1.0000 - val_loss: 1.2132 - val_acc: 0.8556\n",
      "Epoch 22/25\n",
      "20000/20000 [==============================] - 104s 5ms/step - loss: 2.7176e-05 - acc: 1.0000 - val_loss: 1.2417 - val_acc: 0.8568\n",
      "Epoch 23/25\n",
      "20000/20000 [==============================] - 105s 5ms/step - loss: 2.0938e-05 - acc: 1.0000 - val_loss: 1.2675 - val_acc: 0.8566\n",
      "Epoch 24/25\n",
      "20000/20000 [==============================] - 100s 5ms/step - loss: 1.6424e-05 - acc: 1.0000 - val_loss: 1.2884 - val_acc: 0.8560\n",
      "Epoch 25/25\n",
      "20000/20000 [==============================] - 99s 5ms/step - loss: 1.3043e-05 - acc: 1.0000 - val_loss: 1.3074 - val_acc: 0.8560\n"
     ]
    }
   ],
   "source": [
    "from keras import models \n",
    "from keras.layers import Embedding, Dense, LSTM\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(Embedding(20000, 128))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "hist = model.fit(x_train, y_train, epochs=25, batch_size=64, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 87s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "result=model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Test Score(loss)====\n",
      "1.445977435076018\n",
      "====Test Accuracy====\n",
      "0.8438\n"
     ]
    }
   ],
   "source": [
    "print(\"====Test Score(loss)====\")\n",
    "print(result[0])\n",
    "print(\"====Test Accuracy====\")\n",
    "print(result[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
